{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk torch transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import GPTNeoForCausalLM\n",
    "from torch.optim import Adam\n",
    "import sentencepiece\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"EleutherAI/gpt-neo-125M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model_with_context = GPTNeoForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model_with_finetuning = GPTNeoForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_with_context.config.pad_token_id = tokenizer.pad_token_id\n",
    "model_with_finetuning.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_finetuning_copy = GPTNeoForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haikus = list(open('data/haiku.txt').read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model_with_finetuning.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_context_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_context_length)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=len(tokenizer.tokenize(prompt)),  \n",
    "        num_return_sequences=1,\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        repetition_penalty=1.5, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model, text, max_context_length=512, num_epochs=3):\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "        if inputs.input_ids.size(-1) > max_context_length:\n",
    "            inputs.input_ids = inputs.input_ids[:, -max_context_length:]\n",
    "        labels = inputs.input_ids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.input_ids, labels=labels, attention_mask=inputs.attention_mask)\n",
    "        loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(reference, generated_text):\n",
    "    reference_tokens = tokenizer.tokenize(reference)\n",
    "    generated_tokens = tokenizer.tokenize(generated_text)\n",
    "    return sentence_bleu([reference_tokens], generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\" \n",
    "bleu_scores_context = []\n",
    "bleu_scores_finetuning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_haiku(model_with_context, model_with_finetuning, haiku, context=\"\"):\n",
    "    context += haiku + \" \"\n",
    "    \n",
    "    generated_context_full = generate_text(model_with_context, context[-1024:])\n",
    "    generated_finetuning_full = generate_text(model_with_finetuning, context[-1024:])\n",
    "    \n",
    "    bleu_context_full = evaluate_bleu(context, generated_context_full)\n",
    "    bleu_finetuning_full = evaluate_bleu(context, generated_finetuning_full)\n",
    "    \n",
    "    generated_context = generate_text(model_with_context, haiku)\n",
    "    generated_finetuning = generate_text(model_with_finetuning, haiku)\n",
    "    bleu_context = evaluate_bleu(haiku, generated_context)\n",
    "    bleu_finetuning = evaluate_bleu(haiku, generated_finetuning)\n",
    "    \n",
    "    print(f\"Original Haiku: {haiku}\")\n",
    "    print(f\"Generated by Context Model (Haiku): {generated_context} | BLEU: {bleu_context:.4f}\")\n",
    "    print(f\"Generated by Finetuned Model (Haiku): {generated_finetuning} | BLEU: {bleu_finetuning:.4f}\")\n",
    "    print(f\"Generated by Context Model (Full Context): {generated_context_full} | BLEU (Full): {bleu_context_full:.4f}\")\n",
    "    print(f\"Generated by Finetuned Model (Full Context): {generated_finetuning_full} | BLEU (Full): {bleu_finetuning_full:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return context, bleu_context, bleu_finetuning\n",
    "\n",
    "context = \"\" \n",
    "bleu_scores_context = []\n",
    "bleu_scores_finetuning = []\n",
    "\n",
    "for haiku in haikus[:3]:\n",
    "    context, bleu_context, bleu_finetuning = process_haiku(model_with_context, model_with_finetuning, haiku, context)\n",
    "    bleu_scores_context.append(bleu_context)\n",
    "    bleu_scores_finetuning.append(bleu_finetuning)\n",
    "\n",
    "print(\"BLEU Scores for Context Model (Haiku):\", bleu_scores_context)\n",
    "print(\"BLEU Scores for Finetuning Model (Haiku):\", bleu_scores_finetuning)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
